{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class tilecoder:\n",
    "\n",
    "    def __init__(self, numTilings, tiles1d):\n",
    "        self.maxIn = env.observation_space.high\n",
    "        self.minIn = env.observation_space.low\n",
    "        self.numTilings = numTilings\n",
    "        # Define 1-D Size of tiling\n",
    "        self.tiles1d = tiles1d\n",
    "        self.dim = len(self.maxIn)\n",
    "        # In this case with an tiles1d of 18 and numTilings of 4 \n",
    "        # each tile has 324 (18**dim) tiles with a total of 1296 tiles\n",
    "        self.numTiles = (self.tiles1d**self.dim) * self.numTilings\n",
    "        self.actions = env.action_space.n\n",
    "        # Combine these 1296 tiles with 3 possible actions \n",
    "        # and we now have possible 3888 tiles\n",
    "        self.n = self.numTiles * self.actions\n",
    "        # Defines the physical size of the tile based on possible variables\n",
    "        self.tileSize = np.divide(np.subtract(self.maxIn,self.minIn), self.tiles1d-1)\n",
    "\n",
    "    # Takes our current state and returns 4 integers / tile indices\n",
    "    def getFeatures(self, variables):\n",
    "        # Ensures lowest possible input is always 0\n",
    "        self.variables = np.subtract(variables, self.minIn)\n",
    "        tileIndices = np.zeros(self.numTilings)\n",
    "        # Will take in state space and convert into tile indices\n",
    "        matrix = np.zeros([self.numTilings,self.dim])\n",
    "        for i in range(self.numTilings):\n",
    "            for i2 in range(self.dim):\n",
    "                matrix[i,i2] = int(self.variables[i2] / self.tileSize[i2] \\\n",
    "                    + i / self.numTilings)\n",
    "        for i in range(1,self.dim):\n",
    "            matrix[:,i] *= self.tiles1d**i\n",
    "        for i in range(self.numTilings):\n",
    "            tileIndices[i] = (i * (self.tiles1d**self.dim) \\\n",
    "                + sum(matrix[i,:])) \n",
    "        return tileIndices\n",
    "\n",
    "    # Assigns actions values for all possible actions\n",
    "    def getQ(self, features, theta):\n",
    "        Q = np.zeros(self.actions)\n",
    "        for i in range(self.actions):\n",
    "            Q[i] = tile.getVal(theta, features, i)\n",
    "        return Q\n",
    "    \n",
    "    # Calculates action values based upon theta\n",
    "    def getVal(self, theta, features, action):\n",
    "        val = 0 \n",
    "        for i in features:\n",
    "            index = int(i + (self.numTiles*action))\n",
    "            val += theta[index]\n",
    "        return val\n",
    "\n",
    "    # Creates a one hot vector for features so that theta can be updated\n",
    "    def oneHotVector(self, features, action):\n",
    "        oneHot = np.zeros(self.n)\n",
    "        for i in features:\n",
    "            index = int(i + (self.numTiles*action))\n",
    "            oneHot[index] = 1\n",
    "        return oneHot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "tile = tilecoder(4,18)\n",
    "theta = np.random.uniform(-0.001, 0, size=(tile.n))\n",
    "# Custom alpha learned to generalize based upon number of tilings\n",
    "alpha = (.1/ tile.numTilings)*3.2\n",
    "# Discounting not needed since reward gives -1 reward each time step\n",
    "gamma = 1\n",
    "numEpisodes = 10000\n",
    "stepsPerEpisode = 200\n",
    "rewardTracker = []\n",
    "render = True\n",
    "solved = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episodes = 100    Episode Reward = -200.0    Average Reward = -200.0\n",
      "Total Episodes = 200    Episode Reward = -200.0    Average Reward = -199.3\n",
      "Total Episodes = 300    Episode Reward = -200.0    Average Reward = -193.8\n",
      "Total Episodes = 400    Episode Reward = -156.0    Average Reward = -190.9\n",
      "Total Episodes = 500    Episode Reward = -156.0    Average Reward = -187.7\n",
      "Total Episodes = 600    Episode Reward = -146.0    Average Reward = -181.1\n",
      "Total Episodes = 700    Episode Reward = -115.0    Average Reward = -176.3\n",
      "Total Episodes = 800    Episode Reward = -129.0    Average Reward = -171.8\n",
      "Total Episodes = 900    Episode Reward = -191.0    Average Reward = -169.3\n",
      "Total Episodes = 1000    Episode Reward = -107.0    Average Reward = -167.8\n",
      "Total Episodes = 1100    Episode Reward = -134.0    Average Reward = -164.9\n",
      "Total Episodes = 1200    Episode Reward = -157.0    Average Reward = -161.7\n",
      "Total Episodes = 1300    Episode Reward = -161.0    Average Reward = -158.5\n",
      "Total Episodes = 1400    Episode Reward = -142.0    Average Reward = -155.9\n",
      "Total Episodes = 1500    Episode Reward = -87.0    Average Reward = -153.2\n"
     ]
    }
   ],
   "source": [
    "for episodeNum in range(1,numEpisodes+1):\n",
    "    G = 0\n",
    "    state = env.reset()\n",
    "    for step in range(stepsPerEpisode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        F = tile.getFeatures(state)\n",
    "        Q = tile.getQ(F, theta)\n",
    "        action = np.argmax(Q)\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        G += reward\n",
    "        delta = reward - Q[action]\n",
    "        if done == True:\n",
    "            theta += np.multiply((alpha*delta), tile.oneHotVector(F,action))\n",
    "            rewardTracker.append(G)\n",
    "            if episodeNum % 100 == 0:\n",
    "                print(\"Total Episodes = {}    Episode Reward = {}    Average Reward = {:04.1f}\"\\\n",
    "                      .format(episodeNum, G, np.mean(rewardTracker)))\n",
    "            break\n",
    "        Q = tile.getQ(tile.getFeatures(state2), theta)\n",
    "        delta += gamma*np.max(Q)\n",
    "        theta += np.multiply((alpha*delta), tile.oneHotVector(F,action))\n",
    "        state = state2\n",
    "        \n",
    "    if solved != True:\n",
    "        if episodeNum > 100:\n",
    "            if sum(rewardTracker[episodeNum-100:episodeNum])/100 >= -110:\n",
    "                print('Solved in {} Episodes'.format(episodeNum))\n",
    "                render = True\n",
    "                solved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

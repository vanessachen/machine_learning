{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Neural Networks\n",
    "Teacher: Carl Shan\n",
    "\n",
    "Much of the content and ideas in this notebook are taken from Michael Nielsen's excellent [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html) online textbook. Credit goes to Nielsen for the thorough explanations and for many of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Neural Network?\n",
    "A neural network is a machine learning model in which many \"layers\" of \"neurons\" pass information around to teach other in order to produce an output.\n",
    "\n",
    "The output may be a class (in the case of classification or clustering) or a continuous number (in the case of regression).\n",
    "\n",
    "A quick visual depiction of a NN is below:\n",
    "\n",
    "![Neural Network](https://cdn-images-1.medium.com/max/1600/0*IUWJ5oJ_z6AiG7Ja.jpg)\n",
    "[Source](https://cdn-images-1.medium.com/max/1600/0*IUWJ5oJ_z6AiG7Ja.jpg)\n",
    "\n",
    "\n",
    "**To understand Neural Networks, we will begin with understanding the basic building blocks of these models: the neuron.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Neuron?\n",
    "Neural Networks take their name from our biological circuitry inside of our heads. Just as we have brain cells we call \"neurons\" the individiaul components that make up a nueral network are also called Neurons.\n",
    "\n",
    "In this next part of the Jupyter Notebook below, we will be looking at a simple neuron called a **Perceptron**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons: The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Perceptron looks like the following:\n",
    "\n",
    "![Perceptron](http://matlabgeeks.com/wp-content/uploads/2011/05/Perceptron.bmp)\n",
    "[Source](http://matlabgeeks.com/wp-content/uploads/2011/05/Perceptron.bmp)\n",
    "\n",
    "\n",
    "It behaves in this manner: the Perceptron takes in a number of weighted inputs, and depending on whether the sum of these weighted inputs are above or below a chosen threshold, the neuron either \"fires\" or doesn't fire.\n",
    "\n",
    "To fire in this case means to give an output of `1` if the weighted sum is `>=` the threshold, and `0` otherwise.\n",
    "\n",
    "I (Carl) have written a short program below that simulates a Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # importing so we can use the np.dot function\n",
    "\n",
    "def Perceptron(weights, inputs, threshold):\n",
    "    \"\"\"\n",
    "        Returns 1 if weights * inputs >= threshold, else 0\n",
    "    \"\"\"\n",
    "    weighted_sum = np.dot(weights, inputs)\n",
    "    if weighted_sum >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `np.dot(a, b)` returns the dot product of the vectors `a` and `b`. Notice that the dot product of the vector of weights and the vector of inputs is simpy their weighted sum. \n",
    "\n",
    "For example, if you were trying to calculate your grade in a class, and you had gotten the following scores:\n",
    "\n",
    "* 80% on tests\n",
    "* 90% on projects\n",
    "* 70% on homework\n",
    "\n",
    "And your teacher told you that the categories were weighted in the following manner:\n",
    "\n",
    "* tests were 50% of your grade\n",
    "* projects were 30%\n",
    "* homework was 20%\n",
    "\n",
    "Then to calculate your final grade you would calculate:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ final\\_grade = weight\\_tests * tests + weight\\_homework * homework + weight\\_projects * project\n",
    "\\end{equation*}\n",
    "\n",
    "Which in this case would be:\n",
    "`final_grade = 0.5*0.8 + 0.2*0.7 + 0.3*0.9`\n",
    "\n",
    "Notice that this is the same equation as if you had taken the dot product of the vector of weights `[0.5, 0.2, 0.3]` with the vector of scores `[0.8, 0.7, 0.9]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you think the following code will return?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "weights = [0.5, 0.2, 0.3]\n",
    "inputs = [1, 0, 0]\n",
    "thresh = 0.5\n",
    "\n",
    "print(Perceptron(weights, inputs, thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1, 2, 3]\n",
    "inputs = [4, 2, 1]\n",
    "thresh = 12\n",
    "\n",
    "print(Perceptron(weights, inputs, thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Below from [Michael Nielsen's explanation](http://neuralnetworksanddeeplearning.com/chap1.html))\n",
    "\n",
    "That's all there is to how a perceptron works!\n",
    "\n",
    "That's the basic mathematical model. A way you can think about the perceptron is that it's a device that makes decisions by weighing up evidence. Let me give an example. It's not a very realistic example, but it's easy to understand, and we'll soon get to more realistic examples. Suppose the weekend is coming up, and you've heard that there's going to be a cheese festival in your city. You like cheese, and are trying to decide whether or not to go to the festival. You might make your decision by weighing up three factors:\n",
    "\n",
    "1. Is the weather good?\n",
    "2. Does your boyfriend or girlfriend want to accompany you?\n",
    "3. Is the festival near public transit? (You don't own a car).\n",
    "\n",
    "We can represent these three factors by corresponding binary variables `x1`,`x2`, and `x3`. For instance, we'd have `x1=1` if the weather is good, and `x1=0` if the weather is bad. Similarly, `x2=1` and `x2=1` if your boyfriend or girlfriend wants to go, and `x2=0` and `x2=0` if not. And similarly again for `x3` and public transit.\n",
    "\n",
    "Now, suppose you absolutely adore cheese, so much so that you're happy to go to the festival even if your boyfriend or girlfriend is uninterested and the festival is hard to get to. But perhaps you really loathe bad weather, and there's no way you'd go to the festival if the weather is bad. You can use perceptrons to model this kind of decision-making. One way to do this is to choose a weight `w1=6` for the weather, and `w2=2` and `w3=2` for the other conditions. The larger value of `w1` indicates that the weather matters a lot to you, much more than whether your boyfriend or girlfriend joins you, or the nearness of public transit. Finally, suppose you choose a threshold of `5` for the perceptron. \n",
    "\n",
    "With these choices, the perceptron implements the desired decision-making model, outputting `1` whenever the weather is good, and `0` whenever the weather is bad. It makes no difference to the output whether your boyfriend or girlfriend wants to go, or whether public transit is nearby.\n",
    "\n",
    "By varying the weights and the threshold, we can get different models of decision-making. For example, suppose we instead chose a threshold of `3`. Then the perceptron would decide that you should go to the festival whenever the weather was good or when both the festival was near public transit and your boyfriend or girlfriend was willing to join you. In other words, it'd be a different model of decision-making. Dropping the threshold means you're more willing to go to the festival.\n",
    "\n",
    "Obviously, the perceptron isn't a complete model of human decision-making! But what the example illustrates is how a perceptron can weigh up different kinds of evidence in order to make decisions. And it should seem plausible that a complex network of perceptrons could make quite subtle decisions:\n",
    "\n",
    "![Perceptron Network](http://neuralnetworksanddeeplearning.com/images/tikz1.png)\n",
    "[Source](http://neuralnetworksanddeeplearning.com/images/tikz1.png)\n",
    "\n",
    "In this network above, the first column of perceptrons - what we'll call the first layer of perceptrons - is making three very simple decisions, by weighing the input evidence. \n",
    "\n",
    "What about the perceptrons in the second layer? Each of those perceptrons is making a decision by weighing up the results from the first layer of decision-making. In this way a perceptron in the second layer can make a decision at a more complex and more abstract level than perceptrons in the first layer. And even more complex decisions can be made by the perceptron in the third layer. In this way, a many-layer network of perceptrons can engage in sophisticated decision making.\n",
    "\n",
    "Incidentally, when I defined perceptrons I said that a perceptron has just a single output. In the network above the perceptrons look like they have multiple outputs. In fact, they're still single output. The multiple output arrows are merely a useful way of indicating that the output from a perceptron is being used as the input to several other perceptrons. It's less unwieldy than drawing a single output line which then splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Problem with Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are great, but they pose one significant problem when it comes to training out network: \n",
    "> \"[A] small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 00 to 11. That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way.\" \n",
    "> * [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "\n",
    "For an example of this in action, take a look at this Perceptron below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "weights = [1, 2, 3]\n",
    "inputs = [4, 2, 1]\n",
    "thresh = 11\n",
    "\n",
    "print(Perceptron(weights, inputs, thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "weights = [0.75, 2, 3]\n",
    "inputs = [4, 2, 1]\n",
    "thresh = 11\n",
    "\n",
    "print(Perceptron(weights, inputs, thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a tiny change in the first weight completely flips the output of our Neuron. That's because Perceptrons can only output one of two choices: `0` or `1`.\n",
    "\n",
    "This makes it hard for us to train our network since tiny changes in the weights can cause large \"butterfly\" effects throughout the rest of the network in unpredictable ways. This makes the \"learning\" part of machine learning hard. We don't want models that are very high variance!\n",
    "\n",
    "What we want instead is to have neurons that allow us to see what happens when we change the parameters of our network in small ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the Sigmoid Neuron (or \"Logistic\" Neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights cause only a small change in their output. That's the crucial fact which will allow a network of sigmoid neurons to learn.\n",
    "\n",
    "Okay, let me describe the sigmoid neuron. We'll depict sigmoid neurons in the same way we depicted perceptrons:\n",
    "\n",
    "![Sigmoid 1](http://neuralnetworksanddeeplearning.com/images/tikz9.png)\n",
    "[Source](http://neuralnetworksanddeeplearning.com/images/tikz9.png)\n",
    "\n",
    "Just like a perceptron, the sigmoid neuron has inputs, `x1, x2, …`. But instead of being just `0` or `1`, these inputs can also take on any values between `0` and `1`. So, for instance, `0.638…` is a valid input for a sigmoid neuron. Also just like a perceptron, the sigmoid neuron has weights for each input, `w1,w2,…`, and an overall bias, `b`. But the output is not `0` or `1`. Instead, it's `σ(w⋅x+b)`, where `σ` is called the *Sigmoid function*.\n",
    "\n",
    "You already saw the Sigmoid function back when we were doing logistic regression.\n",
    "\n",
    "The sigmoid function is defined in the following way:\n",
    "\n",
    "$σ(z)≡\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "At first sight, sigmoid neurons appear very different to perceptrons. The algebraic form of the sigmoid function may seem opaque and forbidding if you're not already familiar with it. In fact, there are many similarities between perceptrons and sigmoid neurons, and the algebraic form of the sigmoid function turns out to be more of a technical detail than a true barrier to understanding.\n",
    "\n",
    "To understand the similarity to the perceptron model, suppose `z≡w⋅x+b` is a large positive number. Then `e−z≈0` and so `σ(z)≈1`. In other words, when `z=w⋅x+b` is large and positive, the output from the sigmoid neuron is approximately 11, just as it would have been for a perceptron. Suppose on the other hand that `z=w⋅x+b` is very negative. Then `e−z→∞`, and `σ(z)≈0`. So when `z=w⋅x+b` is very negative, the behaviour of a sigmoid neuron also closely approximates a perceptron. It's only when `w⋅x+b` is of modest size that there's much deviation from the perceptron model.\n",
    "\n",
    "What about the algebraic form of `σ`? How can we understand that? In fact, the exact form of `σ` isn't so important - what really matters is the shape of the function when plotted. Here's the shape:\n",
    "\n",
    "![Sigmoid](https://qph.ec.quoracdn.net/main-qimg-05edc1873d0103e36064862a45566dba)\n",
    "[Source](https://qph.ec.quoracdn.net/main-qimg-05edc1873d0103e36064862a45566dba)\n",
    "\n",
    "Notice that this is simply a smoothed out version of the Step Function (when a function only returns `0` or `1`):\n",
    "\n",
    "![Comparison](https://i.stack.imgur.com/I2rWE.png)\n",
    "[Source](https://i.stack.imgur.com/I2rWE.png)\n",
    "\n",
    "So in other words, the Sigmoid Neuron is just a *smoother* version of the Perceptron. It can return continuous values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does a Sigmoid Neuron output?\n",
    "\n",
    "How should we interpret the output from a sigmoid neuron? Obviously, one big difference between perceptrons and sigmoid neurons is that sigmoid neurons don't just output `0` or `1`. They can have as output any real number between `0` and `1`, so values such as `0.173…` and `0.689…` are legitimate outputs. This can be useful, for example, if we want to use the output value to represent the average intensity of the pixels in an image input to a neural network. But sometimes it can be a nuisance. Suppose we are classifying images of handwritten digits and want to train our network to detect what number an image is. We want the output from the network to indicate either \"the input image is a 9\" or \"the input image is not a 9\". \n",
    "\n",
    "Obviously, it'd be easiest to do this if the output was a `0` or a `1`, as in a perceptron. But in practice we can set up a convention to deal with this, for example, by deciding to interpret any output of at least `0.5` as indicating a \"9\", and any output less than `0.5` as indicating \"not a 9\". I'll always explicitly state when we're using such a convention, so it shouldn't cause any confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "A. Write a function called Sigmoid that takes in two inputs: weights and inputs and returns the sigmoid equation of the weighted sum. I've started with a skeleton function for you to use.\n",
    "\n",
    "```python\n",
    "def sigmoid_function(n):\n",
    "    return 1 / (1 + np.exp(-n))\n",
    "\n",
    "def SigmoidNeuron(weights, inputs):\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    NOTE: Remember to use the `return` keyword to actually return a value. Don't just print it.\n",
    "    \"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "B. Check your understanding by thinking about the following situation. Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, `c` where `c > 0`. Show that the behaviour of the network doesn't change (e.g., the final outputs are the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "def sigmoid_function(n):\n",
    "    \"\"\"\n",
    "    You don't need to modify this function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting Neurons together: the architecture of Neural Networks\n",
    "Taken from [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap1.html#the_architecture_of_neural_networks)\n",
    "\n",
    "In the next section I'll introduce a neural network that can do a pretty good job classifying handwritten digits. In preparation for that, it helps to explain some terminology that lets us name different parts of a network. Suppose we have the network:\n",
    "\n",
    "![NN Layers](http://neuralnetworksanddeeplearning.com/images/tikz10.png)\n",
    "[Source](http://neuralnetworksanddeeplearning.com/images/tikz10.png)\n",
    "\n",
    "\n",
    "As mentioned earlier, the leftmost layer in this network is called the *input layer*, and the neurons within the layer are called *input neurons*. The rightmost or *output layer* contains the *output neurons*, or, as in this case, a single output neuron. The middle layer is called a *hidden layer*, since the neurons in this layer are neither inputs nor outputs. \n",
    "\n",
    "The term \"hidden\" perhaps sounds a little mysterious - the first time I heard the term I thought it must have some deep philosophical or mathematical significance - but it really means nothing more than \"not an input or an output\". The network above has just a single hidden layer, but some networks have multiple hidden layers. For example, the following four-layer network has two hidden layers:\n",
    "\n",
    "![Hidden Layers](http://neuralnetworksanddeeplearning.com/images/tikz11.png)\n",
    "[Source](http://neuralnetworksanddeeplearning.com/images/tikz11.png)\n",
    "\n",
    "Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called *multilayer perceptrons* or *MLPs*, despite being made up of sigmoid neurons, not perceptrons. I'm not going to use the MLP terminology in this book, since I think it's confusing, but wanted to warn you of its existence.\n",
    "\n",
    "The design of the input and output layers in a network is often straightforward. For example, suppose we're trying to determine whether a handwritten image depicts a \"9\" or not. A natural way to design the network is to encode the intensities of the image pixels into the input neurons. If the image is a `64` by `64` greyscale image, then we'd have `4,096=64×64` input neurons, with the intensities scaled appropriately between `0` and `1`. The output layer will contain just a single neuron, with output values of less than `0.5` indicating \"input image is not a 9\", and values greater than `0.5` indicating \"input image is a 9 \".\n",
    "\n",
    "While the design of the input and output layers of a neural network is often straightforward, there can be quite an art to the design of the hidden layers. In particular, it's not possible to sum up the design process for the hidden layers with a few simple rules of thumb. Instead, neural networks researchers have developed many design heuristics for the hidden layers, which help people get the behaviour they want out of their nets. For example, such heuristics can be used to help determine how to trade off the number of hidden layers against the time required to train the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Neural Networks\n",
    "\n",
    "Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called *feedforward neural networks*. This means there are no loops in the network - information is always fed forward, never fed back. If we did have loops, we'd end up with situations where the input to the σσ function depended on the output. That'd be hard to make sense of, and so we don't allow such loops.\n",
    "\n",
    "However, there are other models of artificial neural networks in which feedback loops are possible. These models are called [*recurrent neural networks*](https://en.wikipedia.org/wiki/Recurrent_neural_network). The idea in these models is to have neurons which fire for some limited duration of time, before becoming quiescent. That firing can stimulate other neurons, which may fire a little while later, also for a limited duration. That causes still more neurons to fire, and so over time we get a cascade of neurons firing. Loops don't cause problems in such a model, since a neuron's output only affects its input at some later time, not instantaneously.\n",
    "\n",
    "Recurrent neural nets have been less influential than feedforward networks, in part because the learning algorithms for recurrent nets are (at least to date) less powerful. But recurrent networks are still extremely interesting. They're much closer in spirit to how our brains work than feedforward networks. And it's possible that recurrent networks can solve important problems which can only be solved with great difficulty by feedforward networks. However, to limit our scope, in this portion of the lesson we're going to concentrate on the more widely-used feedforward networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Neural Network\n",
    "\n",
    "Okay, so now we have gotten down the idea of a Neuron and that a Neural Network is composed of many layers of neurons that feed into each other. How in the world do we train our network?\n",
    "\n",
    "Well just like before, we will define a cost function and find the parameters of our neural network in a way that minimizes this cost function.\n",
    "\n",
    "The \"parameters\" of our model in this case are the weights and thresholds (or *biases*) of each of the layers of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the weights to \"feed\" data from one layer to the next\n",
    "\n",
    "Say we have the following network:\n",
    "\n",
    "![Example NN](https://i.imgur.com/yE88Ryt.png)\n",
    "[Source](https://i.imgur.com/yE88Ryt.png)\n",
    "\n",
    "Can you see how the first layer produces a weighted sum that gets fed to each of the neurons for the next layer?\n",
    "\n",
    "For example, take the first neuron in the second layer. The weighted sum that gets fed into it is:\n",
    "\n",
    "`0.8 * 1 + 0.2 * 1 = 1`\n",
    "\n",
    "So the weighted sum is `1`.\n",
    "\n",
    "The sigmoid of this weighted sum of `1` is then the input that will be fed into the final layer. \n",
    "\n",
    "The sigmoid of `1` is roughly `0.73`.\n",
    "\n",
    "Verify this by calling the `sigmoid_function` that I defined earlier with the input `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Matrix Multiplication\n",
    "\n",
    "Remember how last week we discussed matrix multiplication? We're going to revisit this topic to see mathematically how we can set up the equation to convert the outputs from one layer into the netputs for the next layer.\n",
    "\n",
    "If you forgot how to multiply matrices, use one of the resources to brush up on your knowledge:\n",
    "\n",
    "* [Matrix Multiplication Visualized](http://matrixmultiplication.xyz/)\n",
    "* [6 min Khan Academy Video](https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro)\n",
    "* [Short Intro to Matrix Multiplication](http://www.anandavala.info/SMN/matrix_multiplication.html)\n",
    "\n",
    "\n",
    "#### Exercises\n",
    "1. After learning about matrix multiplication, can you fill in the blanks to the equation below given the network that you saw above?\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{vmatrix}\n",
    "                ? &  ? \\\\\n",
    "                ? &  ? \\\\\n",
    "                ? &  ? \\\\\n",
    "\\end{vmatrix} * \\left\\lgroup \\matrix{1 \\cr 1} \\right\\rgroup = \\left\\lgroup \\matrix{1 \\cr 1.3 \\cr 0.8} \\right\\rgroup\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use this code cell as \"draft paper\" to figure out the answer to the above problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to get the final output of the middle layer, we need to take the sigmoid of:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left\\lgroup \\matrix{1 \\cr 1.3 \\cr 0.8} \\right\\rgroup\n",
    "\\end{equation*}\n",
    "\n",
    "So, by taking the sigmoid we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "Sigmoid(\\left\\lgroup \\matrix{1 \\cr 1.3 \\cr 0.8} \\right\\rgroup) = \\left\\lgroup \\matrix{0.73 \\cr 0.79 \\cr 0.69} \\right\\rgroup\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the way that we use matrix multiplication to get the inputs to the following layer is to set things up in the following way:\n",
    "\n",
    "\\begin{equation*}\n",
    "weightsMatrix * inputsVector = outputsVector\n",
    "\\end{equation*}\n",
    "\n",
    "Expanding upon the above equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{vmatrix}\n",
    "                w_1 &  w_2 & ... & w_l\\\\\n",
    "                w_3 &  w_4 & ... & ... \\\\\n",
    "                w_5 &  w_6 & ... & ... \\\\\n",
    "                ... &  ... & ... & ... \\\\\n",
    "\\end{vmatrix} * \\left\\lgroup \\matrix{x_1 \\cr x_2 \\cr ... \\cr x_l} \\right\\rgroup = \\left\\lgroup \\matrix{z_1 \\cr z_2 \\cr ... \\cr z_k} \\right\\rgroup\n",
    "\\end{equation*}\n",
    "\n",
    "Where the vector of `[x1, ..., x_l]` is the input to the current layer.\n",
    "\n",
    "The matrix of weights has the following properties:\n",
    "* the *first column* (`[w1, w3, w5 ... ]`) in the matrix are the weights of the *first neuron* in the current layer that connects this *first neuron* to every single neuron in the *second layer*\n",
    "* the *second column* (`[w2, w4, w6 ... ]`) in the matrix are the weights of the *second neuron* in the current layer that connects this *second neuron* to every single neuron in the *second layer*\n",
    "* the *third column* in the matrix are the weights of the *third neuron* in the current layer that connects this *third neuron* to every single neuron in the *second layer*\n",
    "* ... and so on\n",
    "\n",
    "Then, after we get the output:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left\\lgroup \\matrix{z_1 \\cr z_2 \\cr ... \\cr z_k} \\right\\rgroup\n",
    "\\end{equation*}\n",
    "\n",
    "We need to take the sigmoid of this to get the inputs to the next layer.\n",
    "\n",
    "\\begin{equation*}\n",
    "Sigmoid(\\left\\lgroup \\matrix{z_1 \\cr z_2 \\cr ... \\cr z_k} \\right\\rgroup)\n",
    "\\end{equation*}\n",
    "\n",
    "Then this vector of outputs of the *second layer* become in the inputs to the *third layer* and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay, now we know how to frame data passing through our network as a series of matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To summarize what you've learned**: Feed-forward neural networks pass data from the first *input layer* through multiple *hidden layers* (that can be all sorts of different sizes) until it finally outputs in the final *output layer*.\n",
    "\n",
    "Each row in original data is passed through the network and manipulated in three key ways during **every single layer**:\n",
    "1. the row is multiplied by some weight\n",
    "2. the row is combined with other neurons' weighted sum of other rows\n",
    "3. the combination is then \"squashed\" between 0 and 1 through the sigmoid function\n",
    "\n",
    "That is the core idea behind the neural network architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next few classes, we will learn about the following topics:\n",
    "#### 1. Gradient Descent\n",
    "How does a neural network \"learn\" the \"best\" weights for each of the layers?\n",
    "\n",
    "#### 2. Stochastic Gradient Descent\n",
    "How can this learning be made computationally feasible?\n",
    "\n",
    "#### 3. Backpropagation\n",
    "How do we quickly compute the \"gradient\" (AKA derivative) of the \"cost function\" for our neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, start working through the Neural Network assignment that is on Canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Neural Network Tutorial\n",
    "Teacher: Carl Shan\n",
    "\n",
    "Before running this Jupyter Notebook, make sure you've run the requisite commands to install tensorflow and keras.\n",
    "\n",
    "If you haven't, here are the commands you need to run in Terminal:\n",
    "\n",
    "* `sudo easy_install -U pip` - This will make sure you have the latest version of pip installed\n",
    "* `sudo pip3 install tensorflow` - Installing tensorflow\n",
    "* `sudo pip3 install keras` - installing keras\n",
    "\n",
    "\n",
    "\n",
    "This notebook consists of two parts:\n",
    "\n",
    "* **Part 1**: Showing how to set up and use Keras\n",
    "* **Part 2**: Using Keras on a multiclass classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Showing how to set up and use Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializinga  model\n",
    "model = Sequential()\n",
    "\n",
    "# The first layer of your network needs to know the input shape\n",
    "input_shape = (784, )\n",
    "\n",
    "# Adding additional layers\n",
    "model.add(Dense(units=32, input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compiling the Model\n",
    "\n",
    "[According to the Keras tutorial](https://keras.io/getting-started/sequential-model-guide/): \n",
    "\n",
    "*Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:*\n",
    "\n",
    "1. **An optimizer**. This could be the string identifier of an existing optimizer (such as `'rmsprop'` or `'adagrad'`), or an instance of the `Optimizer` class. See this link to all: [optimizers](https://keras.io/optimizers/).\n",
    "2. **A loss function**. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `'categorical_crossentropy'` or `'mse'`), or it can be an objective function. See: losses.\n",
    "3. **A list of metrics**. For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function.*\n",
    "\n",
    "\n",
    "*Below are some examples of how you can compile a model with different parameters.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: For a multi-class classification problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: For a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: For a mean squared error regression problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  `.fit` function. Read its [documentation here](https://keras.io/models/sequential/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training a Neural Network on the Reuters Newswire data\n",
    "Source: [Official Keras Github Repo](https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 3s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Setting the hyperparameters\n",
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "# Downloading the Reuters data\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset\n",
    "If you are curious about the Reuters dataset, see [this link in the official Keras documentation](https://keras.io/datasets/#reuters-newswire-topics-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some details:\n",
    "\n",
    "> The Reuters dataset is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).\n",
    "\n",
    "> Each newswire is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "> As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the loaded data\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 43,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 25,\n",
       " 207,\n",
       " 270,\n",
       " 5,\n",
       " 2,\n",
       " 111,\n",
       " 16,\n",
       " 369,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 7,\n",
       " 89,\n",
       " 5,\n",
       " 19,\n",
       " 102,\n",
       " 6,\n",
       " 19,\n",
       " 124,\n",
       " 15,\n",
       " 90,\n",
       " 67,\n",
       " 84,\n",
       " 22,\n",
       " 482,\n",
       " 26,\n",
       " 7,\n",
       " 48,\n",
       " 4,\n",
       " 49,\n",
       " 8,\n",
       " 864,\n",
       " 39,\n",
       " 209,\n",
       " 154,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 83,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 155,\n",
       " 11,\n",
       " 15,\n",
       " 7,\n",
       " 48,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 504,\n",
       " 6,\n",
       " 258,\n",
       " 6,\n",
       " 272,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 134,\n",
       " 44,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 197,\n",
       " 2,\n",
       " 90,\n",
       " 67,\n",
       " 52,\n",
       " 29,\n",
       " 209,\n",
       " 30,\n",
       " 32,\n",
       " 132,\n",
       " 6,\n",
       " 109,\n",
       " 15,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each element in x_train corresponds to a word index.\n",
    "# For instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 46 total classes (e.g., newswire topics) our network will learn to classify\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building, compiling and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Compiling model...\n",
      "Fitting model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 2s 250us/step - loss: 1.4297 - acc: 0.6786 - val_loss: 1.0913 - val_acc: 0.7642\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 1s 185us/step - loss: 0.7889 - acc: 0.8183 - val_loss: 0.9419 - val_acc: 0.7887\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 1s 178us/step - loss: 0.5501 - acc: 0.8654 - val_loss: 0.8944 - val_acc: 0.7998\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 1s 174us/step - loss: 0.4165 - acc: 0.8983 - val_loss: 0.8797 - val_acc: 0.8053\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 1s 172us/step - loss: 0.3247 - acc: 0.9164 - val_loss: 0.9013 - val_acc: 0.8009\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print('Compiling model...')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Fitting model...')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 54us/step\n",
      "Test score: 0.886729920218\n",
      "Test accuracy: 0.791184327694\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
